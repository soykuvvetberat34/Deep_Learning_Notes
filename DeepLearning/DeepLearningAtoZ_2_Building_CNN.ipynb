{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#12 bin köpek ve kedi verisi içeren bir dataset kullanarak görselin köpek mi kedi mi olduğunu tahmin etmek için kullandığım cnn mimarisi"
      ],
      "metadata": {
        "id": "BjOwCoenjNNd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jTxgdXPvjI_W"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QPc-paFmvvP"
      },
      "source": [
        "drive dan verileri çekme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ExZQJ66jDby",
        "outputId": "98465fb2-3b4c-494a-f1c3-1b5a9e3acf03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6oiJq26Rje1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBiLWP8Bmsto"
      },
      "source": [
        "training set veri ön işleme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAu7V0gAlG77",
        "outputId": "eb0c6c5d-872e-4650-9fd4-d9cdad1265ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8020 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale =1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "training_set=train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/Classroom/single_prediction/training_set',\n",
        "    target_size=(64,64),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_1w6JiBm4vI"
      },
      "source": [
        "test seti ön işleme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGKNOq1Om4Ya",
        "outputId": "6693cf90-6ae0-4f76-dc61-b1c07c542a6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "test_datagen= ImageDataGenerator(rescale=1./255)\n",
        "test_set=test_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/Classroom/test_set',\n",
        "    target_size=(64,64),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDlyBv-4fajA"
      },
      "source": [
        "cnn modeli oluşturma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7FHej_TUfS1t"
      },
      "outputs": [],
      "source": [
        "cnn=tf.keras.models.Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqq7ELKQfgFN"
      },
      "source": [
        "convolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggidmEfkfhlT",
        "outputId": "8a593383-d4e4-4362-aa4a-15a4288f48ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu',input_shape=[64,64,3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Df8MHXYf2Bx"
      },
      "source": [
        "pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KW0mj08Gf5uc"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))#pool_size 2x2 matris şeklinde fmap oluşturacak"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLwL849QgHLy"
      },
      "source": [
        "ikinci convolution katman"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WYkg-4-PgK2w"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5RtuCFAgYE8"
      },
      "source": [
        "flattening"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FHxZUZTNgZmW"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCl_jE6LgjVb"
      },
      "source": [
        "full connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4bHhm6jZgk8H"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=128,activation='relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_5rJ_Hjgp7W"
      },
      "source": [
        "çıktı katmanı"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Wnkwa9V2gyBi"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh5cGfjfirU0"
      },
      "source": [
        "CNN training\n",
        "\n",
        "compile cnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "i_gMR6FsipX-"
      },
      "outputs": [],
      "source": [
        "cnn.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHV39SPXi4yF",
        "outputId": "a7d39645-9b2b-460f-97e2-c09f211bf52a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1660s\u001b[0m 6s/step - accuracy: 0.5228 - loss: 0.7205 - val_accuracy: 0.6515 - val_loss: 0.6301\n",
            "Epoch 2/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 355ms/step - accuracy: 0.6370 - loss: 0.6447 - val_accuracy: 0.6975 - val_loss: 0.5885\n",
            "Epoch 3/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 397ms/step - accuracy: 0.6826 - loss: 0.6033 - val_accuracy: 0.7195 - val_loss: 0.5656\n",
            "Epoch 4/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 354ms/step - accuracy: 0.7074 - loss: 0.5622 - val_accuracy: 0.7295 - val_loss: 0.5422\n",
            "Epoch 5/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 394ms/step - accuracy: 0.7302 - loss: 0.5319 - val_accuracy: 0.7550 - val_loss: 0.4999\n",
            "Epoch 6/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 349ms/step - accuracy: 0.7442 - loss: 0.5097 - val_accuracy: 0.7665 - val_loss: 0.4951\n",
            "Epoch 7/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 358ms/step - accuracy: 0.7660 - loss: 0.4829 - val_accuracy: 0.7710 - val_loss: 0.4804\n",
            "Epoch 8/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 357ms/step - accuracy: 0.7634 - loss: 0.4834 - val_accuracy: 0.7720 - val_loss: 0.4829\n",
            "Epoch 9/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 394ms/step - accuracy: 0.7935 - loss: 0.4376 - val_accuracy: 0.7795 - val_loss: 0.4745\n",
            "Epoch 10/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 387ms/step - accuracy: 0.8091 - loss: 0.4255 - val_accuracy: 0.7660 - val_loss: 0.4956\n",
            "Epoch 11/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 391ms/step - accuracy: 0.8020 - loss: 0.4262 - val_accuracy: 0.7875 - val_loss: 0.4722\n",
            "Epoch 12/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 347ms/step - accuracy: 0.8048 - loss: 0.4246 - val_accuracy: 0.7845 - val_loss: 0.4778\n",
            "Epoch 13/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 354ms/step - accuracy: 0.8100 - loss: 0.4183 - val_accuracy: 0.7910 - val_loss: 0.4606\n",
            "Epoch 14/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 349ms/step - accuracy: 0.8205 - loss: 0.3905 - val_accuracy: 0.7695 - val_loss: 0.5010\n",
            "Epoch 15/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 353ms/step - accuracy: 0.8227 - loss: 0.3892 - val_accuracy: 0.7860 - val_loss: 0.4778\n",
            "Epoch 16/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 403ms/step - accuracy: 0.8364 - loss: 0.3663 - val_accuracy: 0.7960 - val_loss: 0.4726\n",
            "Epoch 17/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 358ms/step - accuracy: 0.8355 - loss: 0.3640 - val_accuracy: 0.7840 - val_loss: 0.4775\n",
            "Epoch 18/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 364ms/step - accuracy: 0.8407 - loss: 0.3502 - val_accuracy: 0.7730 - val_loss: 0.5243\n",
            "Epoch 19/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 364ms/step - accuracy: 0.8488 - loss: 0.3365 - val_accuracy: 0.7870 - val_loss: 0.5121\n",
            "Epoch 20/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 357ms/step - accuracy: 0.8388 - loss: 0.3433 - val_accuracy: 0.8010 - val_loss: 0.4971\n",
            "Epoch 21/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 398ms/step - accuracy: 0.8581 - loss: 0.3141 - val_accuracy: 0.7955 - val_loss: 0.4874\n",
            "Epoch 22/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 392ms/step - accuracy: 0.8689 - loss: 0.3092 - val_accuracy: 0.7975 - val_loss: 0.4966\n",
            "Epoch 23/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 403ms/step - accuracy: 0.8794 - loss: 0.2885 - val_accuracy: 0.8025 - val_loss: 0.4664\n",
            "Epoch 24/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 352ms/step - accuracy: 0.8826 - loss: 0.2895 - val_accuracy: 0.8020 - val_loss: 0.4980\n",
            "Epoch 25/25\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 388ms/step - accuracy: 0.8844 - loss: 0.2682 - val_accuracy: 0.7875 - val_loss: 0.5506\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c085a7418d0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "cnn.fit(x=training_set,validation_data=test_set,epochs=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#örnek tahmin etme"
      ],
      "metadata": {
        "id": "I45flyj58RT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "tensorflow.keras.preprocessing.image modülü, resim verisi okumak, yeniden boyutlandırmak, tensöre dönüştürmek vb. için yardımcı fonksiyonlar içerir."
      ],
      "metadata": {
        "id": "RTuB5CObgoxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "load_img(path, target_size) fonksiyonu, diskten bir resmi açar.\n",
        "\n",
        "path: Yüklemek istediğin resmin dosya yolu. (Burada uzantı .jgp olarak geçilmiş; muhtemelen .jpg olacak.)\n",
        "\n",
        "target_size: Resmi modelin beklediği boyuta (64×64 piksel) ölçekler. Böylece ağın girdi boyutuna uygun hale gelir."
      ],
      "metadata": {
        "id": "H7HlfRtkgx6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "img_to_array fonksiyonu, PIL Image objesini (yüklenen resmi) 3 boyutlu bir NumPy dizisine çevirir:"
      ],
      "metadata": {
        "id": "gRqvifD4g4GK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modeller genellikle birden fazla görüntüyü aynı anda işleyebilmek için “batch” boyutu bekler.\n",
        "\n",
        "expand_dims(..., axis=0) ile dizinin en başına bir öğe daha ekler, böylece şekil (1, 64, 64, 3) olur:\n",
        "\n",
        "1 = bu batch içinde tek bir resim var."
      ],
      "metadata": {
        "id": "c5dh0TXwg9WZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "result dizisi boyutu (1, 1) veya (1, 2) olabilir; modele bağlı olarak farklılık gösterir. Örneğin binary sınıflama için (1,1)."
      ],
      "metadata": {
        "id": "Rj0CiXDRhUAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ".class_indices özelliği, eğitimde kullanılan klasör adlarını (örneğin \"cats\", \"dogs\") integer etiketlere nasıl eşlediğini gösteren bir sözlüktür.\n",
        "\n",
        "Örnek çıktı: {'cats': 0, 'dogs': 1}"
      ],
      "metadata": {
        "id": "p7wutJ6xhxCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "result[0][0]: Batch’in ilk elemanının (tek resmin) model çıktısı.\n",
        "\n",
        "Eğitimde \"dogs\" etiketi için 1, \"cats\" etiketi için 0 atandıysa:\n",
        "\n",
        "== 1 ise model bu resmin köpek olduğunu öngörür.\n",
        "\n",
        "Aksi hâlde kedi olarak sınıflandırır."
      ],
      "metadata": {
        "id": "xtekOkSDioJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "test_image=image.load_img('/content/drive/MyDrive/Classroom/single_prediction/cat_or_dog_1.jpg',target_size=(64,64))\n",
        "test_image=image.img_to_array(test_image)\n",
        "test_image=np.expand_dims(test_image,axis=0)\n",
        "result=cnn.predict(test_image)\n",
        "training_set.class_indices\n",
        "\n",
        "if result[0][0] ==1:\n",
        "  prediction='dog'\n",
        "else:\n",
        "  prediction='cat'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5ge8V_B8TWQ",
        "outputId": "eff5ffce-e716-420a-d09e-e97410d5ed31"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "“Batch” terimi, derin öğrenme modellerinde aynı anda işlenen örnekler kümesini ifade eder. Bir seferde birden fazla resimle (veya veri noktasına) ileri besleme (forward pass) yapabilmek için veriler “küme” (“batch”) halinde modele verilir. Böylece model:\n",
        "\n",
        "Veri Verimliliği ve Hız: Aynı anda, örneğin 32 veya 64 resim üzerinde hesaplama yaparak GPU’nun paralel hesaplama gücünü daha iyi kullanır.\n",
        "\n",
        "İstikrarlı Öğrenme: Her adımda tek bir örnek yerine bir küme üzerinden gradien hesapladığında, öğrenme daha dengeli ve stabil olur.\n",
        "\n",
        "Özetle, batch boyutu modele “kaç örnekle aynı anda” işlem yapacağını söyleyen ilk boyuttur. Tek resim için bile bu boyutun sağlanması gerekir."
      ],
      "metadata": {
        "id": "YLrwyv0fitVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O77NG-Agfg-",
        "outputId": "82a38745-9151-426d-a374-a58b499f6ded"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dog\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}